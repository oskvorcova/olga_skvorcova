# ДОКУМЕНТАЦИЯ

Уникальность данного проекта заключается в том, что приложение для парсинга результатов поисковых запросов может быстро выдавать информацию из разных поисковых систем, а также визуализировать ее, и таким образом можно проводить исследования по странам, изменяя гранулярность времени (за месяц, год и тд).
На примере данного проекта я использовала данное приложение и проанализировала данные о
поисковых запросах некоторых известных в экономических кругах университетов по России и по миру. Вцелом подобный анализ (о нем будет сказано позже) на основе стримлит приложения
можно быстро сделать для любых других данных, в этом и прикол.

# ОПИСАНИЕ ЭТАПОВ ПРОЕКТА:

1. Получение данных из MySQL, по которым мы хотим провести анализ.
   1. Создаем базу данных  - скрипт create_db.sql
   2. Создаем таблицу с университетами - universities_create.sql
   3. Вставляем данные, которые далее обираемся изучить (с разбивкой "страна, английское название университета, русское название университета, дата начала приемной кампании, окончание" и тп). Такую информацию разумно отдельно держать в базе даных, а если мы захотим изучить дополнительно новый университет, то нам будет достаточно просто добавить строку в эту таблицу. Удобно!
   5. Получаем данные из таблиц с помощью Курсора в питоне (Cursor).
   6. Преобразуем данные в списки питона и работаем с ними.
2. Парсинг данных по Яндексу и Google. В процессе парсинга данных по Яндексу оказалось, что Яндекс против того, чтобы его данные собирались - я начала получать бан-капчу от Яндекса,
поэтому данных по Яндексу немного, хотя изначально с помощью Селениума удалось спарсить данные для нескольких университетов.
   1. Для обработки использовался, как было сказано выше, Selenium с веб драйвером Chromium. Мы (селениум бот) заходим на страницу Яндекс трендов, вводим логин, пароль от свой учетной записи и далее вводим название университета в поисковую строку, в ответ получаем на странице браузера html с таблицей с данными, парсим страницу в csv файл в несколько итераций (тк она разделена разными символами) и получаем на выход готовый для анализа csv файл.
   2. В случае с парсингом Гугла мы используем АПИ Гугла, а точнее будет сказать еще более высокую абстракцию в виде библиотеки PyTrends, которая существенно облегчает работу с Google API. Тут все достаточно просто: мы просто проходимся по списку всех университетов, постепенно получая данные в виде DataFrame. Я бы использовала и АПИ Яндекса, чтобы обойтись без Селениума, однако у Яндекс трендов нет АПИ.
3. Проверка данных (что все открывается и работает). Я сделала несколько запросов к полученным данным, сверяем, что все пришло в нужном нам виде.
4. Преобразование данных: так как я хочу еще использовать геопандас, то нужно привести полученные таблицы в вид, который может визуализировать геопандас
   1. Приводим названия стран к общему виду (US - United States of America и тд)
   2. Join-им (строго говоря merge в терминах пандас)
5. Визуализация данных и анализ. Анализ результатов получился очень интересный и полезный как для самих университетов (об этом в мотивации проекта далее), так и для меня лично. Некоторые результаты удивили. 
У меня будет три типа графиков:
   1. популярность конкретного университета с течением времени за год
   2. сравнительная популярность всех
   анализируемых университетов во времени (интерактивный график с помощью plotly) за год (его в динамическом виде можно посмотреть в файле dynamic.html - он не
   отображался в гите (вроде пишут, что это распространенная проблема), поэтому я спецаильно сделала отдельный скриншот и HTML обьект, который можно покрутить и проверить)
   3. сравнительный график популярности университета по странам (за год) с помощью geopandas.
6. Написание документации по проекту.
7. Очистка лишних файлов.


* Также код, с помощью которого я парсила и визуализировала данные (как было сказано ранее) был оформлен в стримлит приложение - чтобы его запустить нужно выполнить инструкцию ниже. Это приложение позволяет:
   1. Смотреть тренды вообще по любому слову во всем мире
   2. Смотреть тренд по странам за разное время
   3. Смотреть тренд по отдельной стране (кроме России, так как похоже, что для RU существует некий бан от Google API)
   4. Анализировать данные в виде DataFrame

# ТЕХНИЧЕСКАЯ СТОРОНА ВОПРОСА:

Проект был разделен на несколько папок и файлов, чтобы соответсвовать культуре чистого кода и стандартам PEP8 (и чтобы набрать баллы за проект :))

**Проект представляет собой**
    
1) university_trends.ipynb - ipynb с исследованием динамики запросов поисковой выдачи некоторых популярных (или не совсем, зато российских) университетов мира
с визуализацией, первичная точка входа в приложение.
2) app.py - Стримлит приложение, что использует тот же код из university_trends.ipynb, и позволяет исследовать динамику по любому слову за любое время по любой стране (как было сказано ранее) 
3) conf.ini - файл с настройками, логинами паролями подключения к базам данных (БД)
4) yandex_data - папка с данными, в которую Селениум складывает спаршенные данные
5) sql - папка, в которой лежат SQL скрипты для работы с БД
6) logs - папка, в которую пишутся логи (но они обычно не отправляются в гит)
7) screenshot_proofs - доказательства правильности работы программы в виде скриншотов и пдф файлов с рабочей программой (вдруг что-то не заработает)
8) chromedriver.exe - хромдрайвер для селениума на винду
9) req.txt - файл с ипользуемыми библиотеками для быстрого развертывания и установки другими людьми (пишут, что это хороший тон программистов)
10) selenium_scrapper.py - скрипт с парсингом данных яндекс через селениум
11) readme.md - файл с документацией

# ИНСТРУКЦИЯ ПО ЗАПУСКУ:

1. pip install -r req.txt - чтобы другие люди, которые захотят использовать этот проект, смогли быстро установить все необходимые библиотеки
2. Необходимо иметь развернутую базу данных MySQL локально или удаленно, неважно, - главное заполнить конфиг для подключения
3. https://chromedriver.chromium.org/ - берем отсюда хромдрайвер для нужной ОС
4. https://www.google.ru/chrome/thank-you.html?statcb=1&installdataindex=empty&defaultbrowser=0
5. streamlit run app.py - запуск Streamlit приложения, о котором говорилось ранее

# МОТИВАЦИЯ ПРОЕКТА:

Данный проект анализирует распределение интернет-запросов о сайтах университетов во времени.
Я посмотрела динамику популярности университетов по интернет-запросам и сравнила
пики популярности и спады с началом и концом приемной кампании в конкретном университете. 
Это исследование может помочь университетам:

1) Сравнить популярность их университета в Сети с другими университетами-конкурентами
2) Рассчитать нагрузку на инфраструктуру их сайта и понять, когда возможно нужно закупить дополнительные серверные мощности
3) Оценить динамику нагрузки во время их приемной кампании с точки зрения человеческих ресурсов
4) Оценить динамику и эффективность их пиар-кампаний (например, проводится пиар-кампания; нужно оценить ее успешность еще до начала приема документов,
чтобы была возможность изменить стратегию; для этого можно проанализировать интернет-запросы этого университета до, во время и после пиар-кампании)
6) Посмотреть, насколько конкретный университет популярен в других странах
7) Примерно оценить, когда ожидать основной поток заявок. Имеется в виду, что если например все начинают смотреть требования и документы к концу приема, залезать на сайт университета, то тогда очевидно и больше заявок будет к концу срока подачи.

А благодаря универсальному стримлит-приложению это исследование обобщается на огромное число других тем.

# ГИПОТЕЗА ПРОЕКТА:

Пики популярности интернет-запросов приходятся на начало или середину приемной кампании университета, когда пользователи уже знают, что подача документов идет,
и поэтому заходят на сайт проверить нюансы и требования. Чем ближе к приемной кампании, тем больше запросов, то есть с наступлением open date приемной кампании
динамика возрастающая и достигает своего пика где-то на середине процесса подачи документов.

# ПЕРСПЕКТИВЫ ПРОЕКТА:

Этот проект может быть улучшен за счет добавления следующих опций:

1) Улучшить работу с БД: создать больше таблиц, добавить связь между ними и установить на отдельнм сервере, а не локально
2) Добавить системы обхода капчи от Яндекса, собрать данные по Яндексу и сравнить их с Гуглом
3) Автоматизировать сбор данных по университетам: можно попробовать добавить ChatGPT, чтобы было легче искать даты по кампаниям университетов. Пока что это делалось вручную, заходя на каждый сайт
4) Исправить "грязные данные", то есть очистить спаршенные данные от параллельных воздействий, мешающих смотреть на связь динамики популярности запросов университетов и датами их приемной кампании
