# ДОКУМЕНТАЦИЯ

Уникальность данного проекта заключается в том что приложение для парсинга результатов поисковых запросов может быстро выдавать информацию из разных поисковых систем, а также визуализировать ее, и таким образом можно проводить исследования по странам, меняя гранулярность времени (за месяц, год и тд).
На примере данного проекта я использовала данное приложение и проанализировала данные о
поисковых запросах некоторых известных в экономических кругах университетов по России и по миру. Вцелом подобный анализ (о нем будет сказано позже) на основе стримлит приложения
можно быстро сделать для любых других данных, в этом и прикол.

# ОПИСАНИЕ ЭТАПОВ ПРОЕКТА:

1. Получение данных из MySQL, по которым мы хотим провести анализ.
   1. Создаем базу данных  - скрипт create_db.sql
   2. Создаем таблицу с университетами - universities_create.sql
   3. Вставляем данные, которые далее обираемся изучить (с разбивкой "страна, английское название университета, русское название университета, дата начала приемной кампании, окончание" и тп). Такую информацию разумно отдельно держать в базе даных, а если мы захотим изучить дополнительно новый университет, то нам будет достаточно просто добавить строку в эту таблицу. Удобно!
   5. Получаем данные из таблиц с помощью Курсора в питоне (Cursor).
   6. Преобразуем данные в списки питона и работаем с ними
2. Парсинг данных по Яндексу и Google. В процессе парсинга данных по Яндексу оказалось, что Яндекс против того, чтобы его данные собирались - я начала получать капчу от Яндекса,
поэтому данных по Яндексу немного, хотя изначально с помощью селениума удалось спарсить данные для нескольки университетов.
   1. Для обработки использовался как было сказано выше selenium с веб драйвером chromium мы (селениум бот) заходим на страницу яндекс тренды вводим логин пароль от свой учетной записи и далее вводим название университета в поисковую строку, в ответ получаем на странице браузера html с таблицей с данными, парсим страницу в csv файл в несколько итераций (тк она разделена разными символами) и получаем на выход готовый для анализа csv файл
   2. В случае с парсингом Гугла мы используем АПИ Гугла, точнее будет сказать еще более высокую абстракцию в виде библиотеки PyTrends которая существенно облегчает работу с Google API, тут все достаточно просто мы банально проходимся по списку всех университетов постепенно получая данные в виде DataFrame. Я бы использовала и АПИ Яндекса чтобы обойтись без селениума однака у Яндекс трендов нет АПИ
3. Проверка данных (что все открывается и работает). я сделала несколько запросов к полученным данным и сверяем что все пришло в нужном нам виде
4. Преобразование данных - тк я хочу еще использовать геопандас то нужно привести полученные таблицы в вид который может визуализировать геопандас
   1. Приводим названия стран к общему виду (US - United States of America итп)
   2. Джойним (строго говоря мерджим в терминах пандас)
5. Визуализация данных и анализ. Есть три типа графиков:
   1. популярность конкретного университета с течением времени за год
   2. сравнительная популярность всех
   анализируемых университетов во времени (интерактивный график с помощью plotly) за год (его в динамическом виде, без запуска ноутбука и просчета можно посмотреть в корне проекта - это файл dynamic.html - конкретно у всех таких динамических графиков есть проблема с отображением в гите, поэтому я спецаильно сделала отдельный скриншот и HTML обьект который можно использовать и проверить)
   3. сравнительный график популярности университета по странам (за год) с помощью geopandas.
6. Написание документации по проекту
7. Очистка лишних файлов


* Так же код с помощью которого я парсила и визуализировала данные (как было сказано ранее) был оформлен в стримлит приложение - чтобы его запустить нужно выполнить инструкцию что представлена ниже. Это приложение позволяет:
   1. Смотреть тренды вообще по любому слову во всем мире
   2. Смотреть тренд по странам за разное время (месяц год пять лет)
   3. Смотреть тренд по отдельной стране (кроме России тк похоже для RU существует некий бан от Google API)
   4. Анализировать данные в виде DataFrame

# ТЕХНИЧЕСКАЯ СТОРОНА ВОПРОСА:

Проект был разделен на несколько папок и файлов - чтобы соответсвовать культуре чистого кода и стандартам PEP8

**Проект представляет собой**
    
1) university_trends.ipynb - ipynb с исследованием динамики запросов поисковой выдачи некоторых популярных (или не совсем, зато российских) университетов мира
с визуализацией, первичная точка входа в приложение.
2) app.py - Стримлит приложение что использует тот же код из university_trends.ipynb и позволяет в моменте исследовать динамику по любому слову за любое время и по стране (как было сказано ранее) 
3) conf.ini - файл с настройками, логинами паролями подключения к БД итп
4) yandex_data - папка с данными в которую селениум складывает спаршенные данные
5) sql - папка в которой лежат  SQL скрипты для работы с БД
6) logs - папка в которую  пишутся логи (но они обычно не отправляются в гит)
7) screenshot_proofs - доказательства правильности работы программы в виде скриншотов и пдф файлов с рабочей программой
8) chromedriver.exe - хромдрайвер для селениума на виндовс
9) req.txt - файл с ипользуемыми библиотеками для быстрого развертывания и устанвки другими людьми
10) selenium_scrapper.py - скрипт с парсингом данных яндекс через селениум
11) readme.md - файл документация

# ИНСТРУКЦИЯ ПО ЗАПУСКУ:

1. pip install -r req.txt - чтобы другие люди что захотят использовать этот проект смогли быстро установить все необходимые библиотеки и зависимости
2. Необходимо иметь развернутую базу данных MySQL локально или удаленно не важно - главное заполнить конфиг для подключения
3. https://chromedriver.chromium.org/ - берем отсюда хромдрайвер для нужной ОС
4. https://www.google.ru/chrome/thank-you.html?statcb=1&installdataindex=empty&defaultbrowser=0
5. streamlit run app.py - запуск Streamlit приложения о котором говорилось ранее

# МОТИВАЦИЯ ПРОЕКТА:

Данный проект анализирует распределение интернет-запросов о сайтах университетов во времени.
Я посмотрю динамику популярности университетов по интернет-запросам и сравню
пики популярности и спады с началом и концом приемной кампании.
Это исследование может помочь университетам:

1) Сравнить то как интересны людям они и их конкуренты
2) Рассчитать нагрузку на инфраструктуру их сайта и окружения
3) Оценить нагрузку во время их приемной кампании с точки зрения человеческих ресурсов
4) Оценить динамику их пиар компаний и их эффективности
5) Посомтреть насколько они популярны в других странах
6) Автоматизировать некоторые свои процессы тем самым высвободив несколько людей от скучной работы
7) Примерно оценить когда ожидать основной поток заявок, ведь возможно все начинают смотреть требования и документы к концу приема, тогда очевидно и больше заявок будет к концу),

А с помощью стримлит приложения и вовсе в моменте посмотреть аналитику буквуально по любой теме, возможно популярности бренда, сравнения с конкурентами итп.

# ГИПОТЕЗА ПРОЕКТА:

Пики популярности интернет-запросов приходятся на начало или середину приемной кампании университета, чем ближе к приемной кампании, тем больше запросов.

# ПЕРСПЕКТИВЫ ПРОЕКТА:

Я собираюсь улучшать этот проект и дальше и хотела бы добавить несколько очевидных и хороших точек роста

1) Добавить новые поисковые системы - например  DuckDuckGo - там очень хорошее и понятное АПИ можно легко интегрировать
2) Улучшить работу С БД - создать больше таблиц добавить связь между ними - установить на отдельнм сервере а не локально
3) Добавить системы обхода капчи от Яндекса
4) Добавить Прокси чтобы  тоже обходить капчу
5) Автоматизировать сбор данных по университетам - может попробовать добавить ChatGPT чтобы было легче искать даты по университетам а не вручную заходя на каждый сайт
